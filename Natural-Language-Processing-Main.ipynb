{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. Natural Lenguage Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speead up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# Let's divide the training and test set into two partitions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "data_train = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "data_val = pd.DataFrame({'text': X_val, 'label': y_val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "# Print some examples of punctuation and stop words\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "\n",
    "# Create a SnowballStemmer object\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Remove inline JavaScript/CSS\n",
    "data_train['preprocessed_text'] = data_train['text'].str.replace(r'<script.*?>.*?</script.*?>', '', flags=re.DOTALL, regex=True)\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.replace(r'<style.*?>.*?</style.*?>', '', flags=re.DOTALL, regex=True)\n",
    "\n",
    "# Remove html comments\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.replace(r'<!--.*?-->', '', flags=re.DOTALL, regex=True)\n",
    "\n",
    "# Remove remaining tags\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.replace(r'<.*?>', '', flags=re.DOTALL, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the special characters\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "\n",
    "# Remove numbers\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# Remove all single characters\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove single characters from the start\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.replace(r'\\^[a-zA-Z]\\s+', ' ', regex=True)\n",
    "\n",
    "# Substituting multiple spaces with single space\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove prefixed 'b'\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.replace(r'^b\\s+', '', regex=True)\n",
    "\n",
    "# Convert to Lowercase\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tame Your Text with Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham messages:\n",
      "\n",
      "Top 10 words in spam messages:\n"
     ]
    }
   ],
   "source": [
    "# Bag Of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the top 10 words in ham and spam messages\n",
    "ham_counts = X_train[data_train['label'] == 'ham'].sum(axis=0).A1\n",
    "spam_counts = X_train[data_train['label'] == 'spam'].sum(axis=0).A1\n",
    "\n",
    "ham_top_words = sorted([(feature_names[i], ham_counts[i]) for i in ham_counts.nonzero()[0]], key=lambda x: x[1], reverse=True)[:10]\n",
    "spam_top_words = sorted([(feature_names[i], spam_counts[i]) for i in spam_counts.nonzero()[0]], key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 words in ham messages:\")\n",
    "for word, count in ham_top_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 words in spam messages:\")\n",
    "for word, count in spam_top_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove inline JavaScript/CSS\n",
    "data_val['preprocessed_text'] = data_val['text'].str.replace(r'<script.*?>.*?</script.*?>', '', flags=re.DOTALL, regex=True)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.replace(r'<style.*?>.*?</style.*?>', '', flags=re.DOTALL, regex=True)\n",
    "\n",
    "# Remove html comments\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.replace(r'<!--.*?-->', '', flags=re.DOTALL, regex=True)\n",
    "\n",
    "# Remove remaining tags\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.replace(r'<.*?>', '', flags=re.DOTALL, regex=True)\n",
    "\n",
    "# Remove all the special characters\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "\n",
    "# Remove numbers\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# Remove all single characters\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove single characters from the start\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.replace(r'\\^[a-zA-Z]\\s+', ' ', regex=True)\n",
    "\n",
    "# Substituting multiple spaces with single space\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove prefixed 'b'\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.replace(r'^b\\s+', '', regex=True)\n",
    "\n",
    "# Convert to Lowercase\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].str.lower()\n",
    "\n",
    "# Remove the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Tame Your Text with Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>1</td>\n",
       "      <td>regard mr nelson smithkindly reply private ema...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>0</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>huma abedin bim checking pat work jack jake re...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>announced monday cant today</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>1</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
       "535  I have not been able to reach oscar this am. W...      0   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
       "557  I can have it announced here on Monday - can't...      0   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "29   regard mr nelson smithkindly reply private ema...           1   \n",
       "535         able reach oscar supposed send pdb receive           1   \n",
       "695  huma abedin bim checking pat work jack jake re...           1   \n",
       "557                        announced monday cant today           1   \n",
       "836  bank africaagence san pedro bp san pedro cote ...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0        75  \n",
       "535                 0        42  \n",
       "695                 0        79  \n",
       "557                 0        27  \n",
       "836                 1      1067  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words)\n",
    "money_simbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list, regex=True, na=False) * 1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words, regex=True, na=False) * 1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list, regex=True, na=False) * 1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words, regex=True, na=False) * 1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x))\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorized training data: (800, 16927)\n",
      "Shape of the vectorized validation data: (200, 16927)\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words with Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Shape of the vectorized training data: {X_train.shape}\")\n",
    "print(f\"Shape of the vectorized validation data: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorized training data: (800, 16927)\n",
      "Shape of the vectorized validation data: (200, 16927)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorize all dataset\n",
    "X_train = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Print the shape of the vectorized dataset\n",
    "print(f\"Shape of the vectorized training data: {X_train.shape}\")\n",
    "print(f\"Shape of the vectorized validation data: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9300\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       125\n",
      "           1       0.85      0.99      0.91        75\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.92      0.94      0.93       200\n",
      "weighted avg       0.94      0.93      0.93       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, data_train['label'])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy = accuracy_score(data_val['label'], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(data_val['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9300\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       125\n",
      "           1       0.85      0.99      0.91        75\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.92      0.94      0.93       200\n",
      "weighted avg       0.94      0.93      0.93       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and MultinomialNB\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(data_train['preprocessed_text'], data_train['label'])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = pipeline.predict(data_val['preprocessed_text'])\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy = accuracy_score(data_val['label'], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(data_val['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this extra task, we're implementing a SPAM/HAM classifier using the Multinomial Naive Bayes classifier with default parameters. We're not allowed to change the classifier,\n",
    "but we can work on finding the best feature representation.\n",
    "#We're creating a pipeline that combines the TfidfVectorizer and the MultinomialNB classifier. The pipeline allows us to chain multiple estimators together and perform the\n",
    "necessary data transformations and model fitting in a single step.\n",
    "#We're fitting the pipeline on the training data using fit, which learns the vocabulary and the parameters of the Naive Bayes model.\n",
    "#Next, we're making predictions on the validation data using predict, which returns the predicted labels (spam or ham) for each text entry.\n",
    "#Finally, we're calculating the accuracy score using accuracy_score from scikit-learn and printing the classification report, which shows the precision, recall, and F1-score \n",
    "for each class.\n",
    "#You can experiment with different feature representations, such as adding or removing the extra features we created earlier (money_mark, suspicious_words, text_len),\n",
    "or trying different preprocessing techniques to improve the performance of the classifier.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
